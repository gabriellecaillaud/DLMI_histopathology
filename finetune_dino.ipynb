{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from lora_finetune import LoraConv,LoraLinear\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy\n",
    "from dataset import BaselineDataset, PrecomputedDataset,precompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = '../train.h5'\n",
    "VAL_IMAGES_PATH = '../val.h5'\n",
    "SEED=0\n",
    "torch.random.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/detectionfeuxdeforet/elyaalaoui_adi/dlmi/dlmi_env/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /raid/home/detectionfeuxdeforet/elyaalaoui_adi/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/raid/home/detectionfeuxdeforet/elyaalaoui_adi/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/raid/home/detectionfeuxdeforet/elyaalaoui_adi/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/raid/home/detectionfeuxdeforet/elyaalaoui_adi/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Working on {device}.')\n",
    "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics on train set\n",
    "\n",
    "mean = [0.7439, 0.5892, 0.7210] \n",
    "std = [0.1717, 0.2065, 0.1664]\n",
    "\n",
    "# Create the normalization transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((98, 98)),\n",
    "            transforms.Normalize(mean=mean, std=std)\n",
    "        ])\n",
    "train_dataset = BaselineDataset(TRAIN_IMAGES_PATH, transform, 'train')\n",
    "val_dataset = BaselineDataset(VAL_IMAGES_PATH, transform, 'train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "train_dataset = PrecomputedDataset(*precompute(train_dataloader, feature_extractor, device))\n",
    "val_dataset = PrecomputedDataset(*precompute(val_dataloader, feature_extractor, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model=deepcopy(feature_extractor)\n",
    "\n",
    "def create_lora_model(model,alpha,r):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            lora_conv = LoraConv(module, r=r,alpha=alpha)\n",
    "            setattr(model, name, lora_conv)\n",
    "        elif isinstance(module,nn.Linear):\n",
    "            lora_lin = LoraLinear(module, r=r,alpha=alpha)\n",
    "            setattr(model, name, lora_lin)\n",
    "        else :\n",
    "            create_lora_model(module,r=r,alpha=alpha)\n",
    "    \n",
    "create_lora_model(lora_model,alpha=6,r=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 384])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img,lab=train_dataset[0]\n",
    "lora_model(img.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlmi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
