{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as F\n",
    "from DLMI_histopathology.train import *\n",
    "from DLMI_histopathology.dataset import *\n",
    "from copy import deepcopy\n",
    "from DLMI_histopathology.lora_finetune import LoraConv, LoraLinear\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGES_PATH = 'train.h5'\n",
    "VAL_IMAGES_PATH = 'val.h5'\n",
    "TEST_IMAGES_PATH = 'test.h5'\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Working on {device}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size=(98,98)\n",
    "mean = [0.7439, 0.5892, 0.7210] \n",
    "std = [0.1717, 0.2065, 0.1664]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = BaselineDataset(TRAIN_IMAGES_PATH, train_transform, 'train')\n",
    "val_dataset = BaselineDataset(VAL_IMAGES_PATH, val_transform, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /raid/home/detectionfeuxdeforet/caillaud_gab/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/raid/home/detectionfeuxdeforet/caillaud_gab/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/raid/home/detectionfeuxdeforet/caillaud_gab/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/raid/home/detectionfeuxdeforet/caillaud_gab/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "\n",
    "for name, param in feature_extractor.named_parameters():\n",
    "    if  \"blocks.11\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_dataloader, val_dataloader, device, optimizer_name='Adam', optimizer_params={'lr': 0.001}, \n",
    "                 loss_name='BCELoss', metric_name='Accuracy', num_epochs=100, patience=10, save_path='best_model.pth'):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model to train.\n",
    "        train_dataloader: DataLoader for training data.\n",
    "        val_dataloader: DataLoader for validation data.\n",
    "        device: Device to run training on ('cuda' or 'cpu').\n",
    "        optimizer_name: Name of the optimizer (e.g., 'Adam').\n",
    "        optimizer_params: Dictionary of optimizer parameters.\n",
    "        loss_name: Name of the loss function (e.g., 'BCELoss').\n",
    "        metric_name: Name of the evaluation metric (e.g., 'Accuracy').\n",
    "        num_epochs: Maximum number of training epochs.\n",
    "        patience: Number of epochs to wait for improvement before stopping.\n",
    "        save_path: Path to save the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), **optimizer_params)\n",
    "    criterion = getattr(torch.nn, loss_name)()\n",
    "    metric = getattr(torchmetrics, metric_name)('binary')\n",
    "    \n",
    "    min_loss, best_epoch = float('inf'), 0\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_metrics, train_losses = [], []\n",
    "        \n",
    "        for train_x, train_y in tqdm(train_dataloader, leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = model(train_x.to(device))\n",
    "            # print(f\"{train_pred=}\")\n",
    "            # print(f\"{train_y=}\")\n",
    "            train_pred = train_pred.squeeze(1)\n",
    "            loss = criterion(train_pred, train_y.to(device).to(torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.extend([loss.item()] * len(train_y))\n",
    "            train_metric = metric(train_pred.cpu(), train_y.int().cpu())\n",
    "            train_metrics.extend([train_metric.item()] * len(train_y))\n",
    "        \n",
    "        model.eval()\n",
    "        val_metrics, val_losses = [], []\n",
    "        \n",
    "        for val_x, val_y in tqdm(val_dataloader, leave=False):\n",
    "            with torch.no_grad():\n",
    "                val_pred = model(val_x.to(device))\n",
    "                val_pred = val_pred.squeeze(1)\n",
    "            loss = criterion(val_pred, val_y.to(device).to(torch.float32))\n",
    "            \n",
    "            val_losses.extend([loss.item()] * len(val_y))\n",
    "            val_metric = metric(val_pred.cpu(), val_y.int().cpu())\n",
    "            val_metrics.extend([val_metric.item()] * len(val_y))\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {np.mean(train_losses):.4f} | Train Metric {np.mean(train_metrics):.4f} '\n",
    "              f'Val Loss: {np.mean(val_losses):.4f} Val Metric {np.mean(val_metrics):.4f}')\n",
    "        \n",
    "        if np.mean(val_losses) < min_loss:\n",
    "            mean_val_loss = np.mean(val_losses)\n",
    "            print(f'New best loss {min_loss:.4f} -> {mean_val_loss:.4f}')\n",
    "            min_loss = mean_val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        \n",
    "        if epoch - best_epoch == patience:\n",
    "            print(f\"Model has not improved in val set for {patience} epochs. Stopping early.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a7ee1a43774c91915523877342ba7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea3214e03d44c0db7f8d868d85f1b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] Train Loss: 0.0870 | Train Metric 0.9715 Val Loss: 0.2258 Val Metric 0.9159\n",
      "New best loss inf -> 0.2258\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96fa00551c54c07a7ae0e4bb5331400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a890769464423993d57ec07776af77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] Train Loss: 0.0653 | Train Metric 0.9776 Val Loss: 0.2769 Val Metric 0.9136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c50af26a29642dd9f4c3d41d5567157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1563 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from DLMI_histopathology.models import HistoClassifierHead\n",
    "\n",
    "classif = HistoClassifierHead(dim_input=384, hidden_dim=64, dropout=0.2)\n",
    "model=nn.Sequential(feature_extractor,classif)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "OPTIMIZER = 'AdamW'\n",
    "OPTIMIZER_PARAMS = {'lr': 5e-4, 'weight_decay' : 0.02}\n",
    "LOSS = 'BCELoss'\n",
    "METRIC = 'Accuracy'\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE = 15\n",
    "optimizer = getattr(torch.optim, OPTIMIZER)(model.parameters(), **OPTIMIZER_PARAMS)\n",
    "criterion = getattr(torch.nn, LOSS)()\n",
    "metric = getattr(torchmetrics, METRIC)('binary')\n",
    "min_loss, best_epoch = float('inf'), 0\n",
    "\n",
    "train_model(model, train_dataloader, val_dataloader, device, optimizer_name=OPTIMIZER, optimizer_params=OPTIMIZER_PARAMS, \n",
    "                 loss_name=LOSS, metric_name=METRIC, num_epochs=NUM_EPOCHS, patience=PATIENCE, save_path='best_model_dino_last_block.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef229db1893b40998b4d3c19525f4732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/85054 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('best_model_dino_last_block.pth', weights_only=True))\n",
    "model.eval()\n",
    "model.to(device)\n",
    "prediction_dict = {}\n",
    "\n",
    "with h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n",
    "    test_ids = list(hdf.keys())\n",
    "     \n",
    "\n",
    "solutions_data = {'ID': [], 'Pred': []}\n",
    "with h5py.File(TEST_IMAGES_PATH, 'r') as hdf:\n",
    "    for test_id in tqdm(test_ids):\n",
    "        img = np.array(hdf.get(test_id).get('img'))\n",
    "        img = val_transform(torch.tensor(img)).unsqueeze(0).float()\n",
    "        img = img.to(device)\n",
    "        pred = model(img).detach().cpu()\n",
    "        solutions_data['ID'].append(int(test_id))\n",
    "        solutions_data['Pred'].append(int(pred.item() > 0.5))\n",
    "solutions_data = pd.DataFrame(solutions_data).set_index('ID')\n",
    "solutions_data.to_csv('results_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): DinoVisionTransformer(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x NestedTensorBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MemEffAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls1): LayerScale()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ls2): LayerScale()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "    (head): Identity()\n",
       "  )\n",
       "  (1): HistoClassifierHead(\n",
       "    (layer1): Linear(in_features=384, out_features=64, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (layer3): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (layernorm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (layernorm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (layernorm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (sigmoid): Sigmoid()\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
